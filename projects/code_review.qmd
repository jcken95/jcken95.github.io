---
  title: "Code review for statisticians, data scientists & modellers"
  subtitle: ""
  description: "todo"
  format: html
  date: 09-11-2024
  about:
    template: marquee
---

# The who, what, why and how of code review

## Who is this for?

I work as a data scientist alongside people who typically have academic backgrounds in statistics, public health, mathematical biology or mathematical ecology. Some of us have job titles relating to data science, other to mathematical and statistical modelling. None of us are software developers trade.

However, we're writing software all day. We're writing mathematical/statistical models, creating data-driven products and doing data engineering. We quality assure all of this work via code review.

The honest answer to _Who is this for?_ is that this is for me. It's for a previous version of myself who knew the mechanics of reviewing code, but could have exercised a bit more nuance. 

That being said, I think anyone working in some kind of modelling or analytical role, whose main output is some sort of mathematical or statistical analysis, but implements their processes via code could find this useful. I'll call these people "analysts".

It doesn't really matter what programming language you use, or whether you're storing your code on GitHub, GitLab, Bitbucket or whatever else. However this will be biased towards R and GitHub because that's what I use. The concepts are important here, not the precise toolkit.

If you're here, I'm going to assume you know roughly what a code review is, but it doesn't hurt to have a quick refresher.

Code review is a form of peer review. Let‚Äôs suppose you‚Äôve written some code, [made a pull request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request) and then assigned a reviewer. The reviewer goes through your changes and looks for potential issues with the code, suggests improvements, and maybe says one or two nice things about the thing you've been grinding away at.


# How to review code

In our team, we provide a formal review via GitHub. Let's suppose you've opened a pull request, marked it as ready and assigned one or more reviewers.

We will first think about the literal act of reviewing code, and then later we will think about what the reviewer should look for and how they should approach requesting changes.

 
## Literal code review

Here you're going to look at the code, maybe run or interact with it, then make comments about the code via the pull request mechanisms.

To make a comment, click the `+` symbol to comment on a single line of code. Click a `+` and drag down to comment on multiple lines.

![image](code_review/plus_symbol.png)

   * You can write a general comment or question in the text box.
   * Click the little `+/-` symbol (it looks a bit like a page üìÑ) to make a direction suggestion on the code. This is useful when it's simply easier to make the fix, than telling someone how to make the fix.
   * Clicking the symbol again will let you make a new suggestion on the _same_ line(s) of code. Handy when you think there are multiple ways forward.

![image](code_review/example_suggest.png)

   * Click `Add single comment` if the review is very short, informal or you're just being nosey. `Start review` is better for longer or more formal reviews.
   * Once you've finished reviewing, click `Review changes` in the top right. You may want to write some feedback in the comment box. Then click one of `Comment`, `Approve` or `Request changes` as appropriate and then `Submit Review`.

   * Comments on code start a "conversation" which can be resolved. It is up to the reviewer to resolve conversations. I'd encourage authors to use emoji reactions or comments to track their work.
   * You can view changes incurred by a particular commit. Clicking the commit ID on the commit history (main page of the PR) will show you these changes. This is really useful in the ping-pong stage of the review to isolate changes.


## What is a reviewer looking for?

When reviewing code, it's a good idea to follow a mental checklist. I find the following works well for me and the team I'm part of:

  * Does the code _actually run_?
  * Can I tell what this code should do?
  * Does the code do what it should do?
  * Does the code have clear, maintainable structure? I think [Chapter 3](https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/code.html) of _Best Coding Practices for R_ is a good guide.
  * Is the code difficult to read, overly complex or full of repetition?
  * Is it clear how to use or run the code?
  * Does the code have a consistent style with evocative function and variable names?
  * Are there any merge conflicts?

There's also a second _What_ here: what does the reviewer _do_ during code review?

#### Asks questions to help the reviewer understand the code.

This is inherently tied to the rest of code review, but I think a good way to review code is via questions. More on this later.

Ask the code author about why they've implemented their methods in a certain way, ask them if they think there are better ways to do things, ask them to clarify chunks of code with unclear meaning, output or purpose and ask if their implementations are robust to future changes.


#### Identifies (potential) problems in the code:

As a reviewer, you need to figure out if the code meets some _absolute minimum_ requirements. To do this, you're going to look for

  * Does the code do what it should do?
  * Bugs (or potential future bugs).
  * Incorrect mathematics/statistics.
  * Have we accidentally pushed secure or sensitive information?   

Anything which is buggy, incorrect or insecure really shouldn't be merged into main.

#### Suggests ways to improve the code:

You should not just be looking for things that are wrong with the code, but also ways to improve the health of the code and your models:

  * Suggest refactors/changes to improve clarity of code or documentation, reproducibility and/or performance.
  * Suggest adding useful features.
  * Suggest removing redundant or excessive features.

 
#### Provides nit picks:

Nit picks, or just nits, are annoying (and something I do too much, sorry colleages), but they are invariably something a reviewer does.

Nits are usally small stylistic points. Examples include:

 * White space.
 * Variable/function names (this is arguably more than a nit!)
 * Positioning of brackets, choice of assignment operator, and other fairly inconsequential suggestions.

In my team, we avoid this with [{lintr}](https://lintr.r-lib.org/), [{styler}](https://www.tidyverse.org/blog/2017/12/styler-1.0.0/) and agreeing on code style as a team. We ensure their usage via precommit hooks; R users may be interested in [{precommit}](https://github.com/lorenzwalthert/precommit).

As a reviewer, try to use nits sparingly. Nits are allowed, but try to keep the number of them small. They‚Äôre not a hill to die on.

It can be a good idea to prefix your nit with ‚Äúnit:‚Äù to make it clear that you're just being a pedant and the code author can probably ignore you.



## Why do we review code?

There is a fairly obvious answer to this question: to check the code does what it should. But there are a whole host of reasons to review code.

#### Knowledge transfer & collaboration

Analysts have a broad range of backgrounds and experiences. The reviewer can learn from the PR author, and by making suggestions, the author can learn from the reviewer by inspecting their code, running it, and asking questions.

Shared knowledge about the code base ensures there is not a single point of failure in the team: you _really_ want your [bus number](https://en.wikipedia.org/wiki/Bus_factor) to be bigger than one.

By reviewing code, the authors and reviewers are brought together to solve a problem. It can even be a good idea to bring in a reviewer before the work is complete to get the author unstuck, or just provide a little guidance.

#### Squash bugs before they occur

We don't want bugs in production code. Checking over the code before it's put into production is the best way to prevent this. But bugs do occur, we're only human.


#### Quality and compliance:

Here you'll be checking that code is maintainable and well formatted. You'll also want to check that the outputs meet a minimum standard, for example, does your model give vaguely sensible predictions?

There's also a security aspect here: by reviewing code, we provide a guard against malice.  This should _never_ be a problem, but nasty people do have jobs, a little code review protects against this.

## How should the reviewers approach review?

We're going to think here about how review code in a way which addresses concerns in a way which is as painless as possible. Those with an academic background will be familiar with _Reviewer 2_. Apoligies for any trauma I've just forced you to relive. We don't want to be like reviewer 2, but there is more to consider.

#### Code review needs to be inclusive

Senior and junior members should be performing code review. Distributing review across the team helps improve knowledge transfer, and gives team members the opportunity to become familiar with large parts of the code base.

This also stops individual members of the team getting swamped with review. If all they do is review code, they're gonning be stroppy and probably provide bad reviews.

#### Do not allow deviations from the team style guide

My team adopted the [tidyverse](https://style.tidyverse.org/) style guide, with some minor tweaks. It doesn't really matter which style guide you use, just pick one that's readable.

Adopting a style guide avoids nits, makes code easier to understand, and because we know what to expect, it ultimately makes everyone‚Äôs life easier.

I think it‚Äôs _okay_ to be pragmatic about the style guide in a rapid response. If you're regularly sticking to the style guide, you'll be using it automatically anyway. But if there are problems, do spin up a second PR which cleans up the code as soon as you can.

If a violation of the style guide is a big fix, because you're needing to quickly make changes to a rarely maintained part of the code base, a separate ticket or issue for this is okay too.

#### Be kind when reviewing code

Sometimes code review can come across as blunt ‚Äì we‚Äôre all really busy, it‚Äôs easier to write short, direct comments. Do try to avoid this, but it can be tricky. Reviewers should try their hardest not to be blunt, but authors should bear this in mind.

You can avoid being blunt by breaking up review sessions. Take your own patience into account, but if you're reviewing for an hour or more, you're probably going to start cutting corners and getting grumpy.

If you find yourself being blunt, grumpy or otherwise unsavoury, [Owen](https://github.com/owenjonesuob) recommends having a cup of tea before finalising your review. I think he's right.

#### Favour asking open ended questions instead of making opinionated, or strong statements

You can offer potential improvements or solutions which could be an improvement, without assuming that your suggestion is the best way forward.

Reviews in this style tend to ask for a clarification or be phrased such that the reviewer is missing some context or knowledge.

#### Tell people what they've done well!

We should really use code review to _praise_ our team members too. I think this is something we (and definitely I) should do more often.

 

### Resolving stalemates

Sometimes a reviewer and author can't agree on what the best solution to a problem is. This can lead to _very_ long games of GitHub ping-pong and grumpy team members. Here are possible scenarios, and the a suggested resolution.

These really are guiding principles rather than strict rules. Use your judgement.

 
#### The reviewer thinks that the PR is mostly good, but isn't perfect

Favour accepting imperfect PRs when when the PR improves the overall quality of:

 * Team outputs.
 * The codebase.

It's generally a good idea to just get things that are "good enough" in main. I'd suggest creating issues/tickets to make the additional improvements.

For example, if you've got a functioning regression model, but there are potentially some missing interaction terms, I'd say get the okayish regression model in main, then investigate interaction terms as a seperate piece of work.

#### The author is overwhelmed with the number of changes

One way to help is for the reviewer could indicate how critical each request is (this could even be done in the first round of review):

  * Nit: very minor change
  * Optional(/consider): I think this is a good idea. It's not a strict requirement.
  * FYI: I'm not expecting this now, it might even be out of scope. I do think you would find this interesting, or would find this to be a better way to tackle problems in the future.
  * Should: an essential fix, unless you can provide me with a compelling reason not to do this.

#### The author thinks a requested change will take too long to fix, or is out of scope

Consider how much fix will improve the overall team output/code health. I'd also encourage the author to try to make the fix anyway. If it takes more than (say) 10 minutes, spin up a new issue to resolve when you have more time.

#### The author and reviewer just can't agree on the best way forward

Deciding factors should be the readability and maintainability of the two competing approaches -- I think this is especially true for analysts. We're mostly writing in high-level languages which aren't super fast.

However, if there _is_ a compelling performance increase in terms of memory or speed, where this is important, that may also be a deciding factor too.

It's also worth setting up a (video) meeting about the code. Communication nuiances can get lost via a written format. Once you've done this, make a note of the main points/actions of the call somewhere in the pull request.

#### Consider bringing in an additional reviewer

Sometimes, two people just really struggle to come to an agreement. I think in this case it's best to just let someone else make the decision.

  * Both give your reasonings to the additional reviewer
  * The second reviewer's decision is the one you go with

 

 

 

Further reading / references

 

* [Google's engineering practices](https://google.github.io/eng-practices/review/reviewer/looking-for.html)

* [Gitlab blog: What is a code review?](https://about.gitlab.com/topics/version-control/what-is-code-review/)

* [Stackoverflow blog: How to Make Good Code Review Better](https://stackoverflow.blog/2019/09/30/how-to-make-good-code-reviews-better/)

* [Tidyteam code review principles](https://code-review.tidyverse.org/ )